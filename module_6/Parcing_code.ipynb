{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Код для парсинга параметров и стоимости автомобилей"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Цель программы\n",
    "Создать тренировочный датасет, используя объявления с сайта auto.ru."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Задачи:\n",
    "1. Собрать данные;\n",
    "2. Выгрузить датасет для дальнейшей обработки."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Импортируем библиотеки необходимые для обработки данных"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd \n",
    "import time\n",
    "import requests as r\n",
    "import json\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.firefox.options import Options"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Выберем модель для сбора данных"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1. Выбор марки автомобилей"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Если начать напрямую брать ссылки с https://auto.ru/moskva/cars/used/, то можно упереться в ограничения сайта на 99 страниц, поэтому нужно выбирать ссылки хитрее. Добавим в ссылку на сайт марку автомобиля."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Можно попытаться выбрать марку автомобиля с сайта:\n",
    "\n",
    "    driver = webdriver.Firefox(executable_path=r'C:\\Program Files\\Mozilla Firefox\\geckodriver.exe')\n",
    "    START_URL = f'https://auto.ru/moskva/cars/used/'\n",
    "    driver.get(START_URL)\n",
    "    START_page = driver.execute_script(\"return document.body.innerHTML;\")\n",
    "    soup = BeautifulSoup(START_page, 'html.parser')\n",
    "    for link in soup.findAll('a',{\"class\": \"Link ListingPopularMMM__itemName\"}):\n",
    "        START_URL_brand.append(link['href'])\n",
    "    driver.quit()\n",
    "    \n",
    "Но результат приносит только 7 самых популярных брендов."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Можно также скачать тестовый датасет:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = 'D:/Python/files/ML/Itog_7/' \n",
    "test = pd.read_csv(path+'test.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Выбрать только нужные марки автомобелей из тестового датасета:\n",
    "\n",
    "    Car_brand = list(test['brand'].unique())\n",
    "    Car_type = [x.lower() for x in Car_brand]\n",
    "    Car_type"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В нашем случае марки автомобилей выбраны вручную:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Car_type = ['bmw', \n",
    "            'mercedes', \n",
    "            'kia', \n",
    "            'hyundai', \n",
    "            'vaz', \n",
    "            'volkswagen', \n",
    "            'toyota', \n",
    "            'nissan',\n",
    "            'ford',\n",
    "            'audi',\n",
    "            'skoda', \n",
    "            'mitsubishi', \n",
    "            'renault', \n",
    "            'mazda', \n",
    "            'chevrolet', \n",
    "            'opel',\n",
    "            'land_rover', \n",
    "            'volvo', \n",
    "            'porsche',\n",
    "            'peugeot', \n",
    "            'infiniti', \n",
    "            'honda', \n",
    "            'daewoo', \n",
    "            'gaz', \n",
    "            'citroen', \n",
    "            'lexus', \n",
    "            'subaru', \n",
    "            'suzuki', \n",
    "            'uaz', \n",
    "            'mini'\n",
    "           ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2. Парсинг ссылок на объявления о продаже"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Создадим список машин и список ссылок на объявления о продаже:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cars_list = []\n",
    "pages_url_list = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Перебирая каждый выбранный брэнд 'Car_type' выбираем страницу с годом выпуска автомобиля 'j' и каждый год пролистываем 6 страниц 'i' для машин до 2005 года выпуска и 15 страниц 'i' - с 2005 года выпуска:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "driver = webdriver.Firefox(executable_path=r'C:\\Program Files\\Mozilla Firefox\\geckodriver.exe')\n",
    "for typ in Car_type:\n",
    "    for j in range(1990,2022):\n",
    "        START_URL = f'https://auto.ru/moskva/cars/'+typ+'/'+str(j)+'-year/used/'\n",
    "        if j < 2005:\n",
    "            n = 7\n",
    "        else:\n",
    "            n = 16\n",
    "        for i in range(1,n):\n",
    "            driver.get(START_URL+'?page='+str(i))\n",
    "            if r.get(START_URL+'?page='+str(i)).status_code != 404:\n",
    "                START_page = driver.execute_script(\"return document.body.innerHTML;\")\n",
    "                soup = BeautifulSoup(START_page, 'html.parser')\n",
    "                for link in soup.findAll('a',{\"class\": \"Link OfferThumb\"}):\n",
    "                    if link['href'] not in pages_url_list:\n",
    "                        pages_url_list.append(link['href'])\n",
    "                        time.sleep(0.2)\n",
    "driver.quit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Также посмотрим много ли в тестовом датасете машин до 90 года выпуска и самый старый автомобиль:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(test['modelDate']<1990).value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test['modelDate'].min()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Если в тестовом датасете есть автомобиль 1904 года перебираем каждый выбранный брэнд 'Car_type' по одной странице с годом выпуска с 1900 по 1990 в регионе 'Россия':"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "driver = webdriver.Firefox(executable_path=r'C:\\Program Files\\Mozilla Firefox\\geckodriver.exe')\n",
    "for typ in Car_type:\n",
    "    for j in range(1900,1990):\n",
    "        START_URL = f'https://auto.ru/rossiya/cars/'+typ+'/'+str(j)+'-year/used/'\n",
    "        driver.get(START_URL+'?page='+str(1))\n",
    "        if r.get(START_URL+'?page='+str(1)).status_code != 404:\n",
    "            START_page = driver.execute_script(\"return document.body.innerHTML;\")\n",
    "            soup = BeautifulSoup(START_page, 'html.parser')\n",
    "            for link in soup.findAll('a',{\"class\": \"Link OfferThumb\"}):\n",
    "                if link['href'] not in pages_url_list:\n",
    "                    pages_url_list.append(link['href'])\n",
    "                    time.sleep(0.2)\n",
    "driver.quit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Осматриваем полученные ссылки:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Page_list = pd.DataFrame(pages_url_list)\n",
    "Page_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Запишем их в датасет:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Page_list.to_csv(path+'page_list.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Этот датасет пригодится на случай потери электроэнергии или интернета. Его всегда можно будет загрузить:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Page_list = pd.read_csv(path+'page_list.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.3. Парсинг данных с объявлений"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ставим флаг, на случай потери соединения можно будет продолжить парсинг с флага. \n",
    "\n",
    "Тут можно задать свой флаг на случай восстановления:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flag = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Смотрим flag и ссылку с который начнется (продолжится) парсинг:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pages_url_list[flag]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "С каждой ссылки пытаемся записать данные в cars_list:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "driver = webdriver.Firefox(executable_path=r'C:\\Program Files\\Mozilla Firefox\\geckodriver.exe')\n",
    "for item in pages_url_list: \n",
    "#for item in pages_url_list[flag:len(pages_url_list)]:\n",
    "    driver.get(item)\n",
    "    page = driver.execute_script(\"return document.body.innerHTML;\")\n",
    "    soup = BeautifulSoup(page, 'html.parser')\n",
    "    response = r.get(item)\n",
    "    response.encoding = 'utf8'\n",
    "    soup_utf = BeautifulSoup(response.text, 'html.parser')\n",
    "    flag = pages_url_list.index(item)\n",
    "    try:\n",
    "        settings_car = json.loads(soup_utf.find('script', type=\"application/ld+json\").string)\n",
    "        more_settings = [\n",
    "        child.get_text(': ').replace('\\xa0', ' ') for child in soup.find('ul',class_='CardInfo').children\n",
    "        ]\n",
    "        for i in range(len(more_settings)):\n",
    "            settings_car[more_settings[i].split(':')[0]] = more_settings[i].split(':')[1]\n",
    "        for key, value in settings_car['offers'].items():\n",
    "            settings_car[key] = value\n",
    "        cars_list.append(settings_car)\n",
    "        print(item)\n",
    "    except:\n",
    "        pass\n",
    "    time.sleep(0.2)\n",
    "driver.quit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Постобработка"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Преобразуем полученные данные в DataFrame:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cars_df = pd.DataFrame(cars_list)\n",
    "cars_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "По уникальной ссылке 'url' избавимся от дубликатов в данных:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cars_df = cars_df.drop_duplicates(subset=['url'])\n",
    "cars_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Запишем полученные данные в файл:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cars_df.to_csv(path+'cars_list.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Выводы:\n",
    "1. На парсинг сайтов уходит очень много времени и нужен хороший стабильный интернет\n",
    "2. Данные сохранены и выгружены датасет для дальнейшей обработки.\n",
    "3. Через несколько дней можно загрузить ранее созданный 'Page_list' дополнить его и спарсить дополнительные данные через flag. 'Page_list' необходим, чтобы не парсить дубликаты благодаря строчке:\n",
    "\n",
    "        if link['href'] not in pages_url_list:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
